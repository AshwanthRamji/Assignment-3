{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll generate your own Simpsons TV scripts using RNNs. You'll be using part of the Simpsons dataset of scripts from 27 seasons. The Neural Network you'll build will generate a new TV script for a scene at Moe's Tavern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwa\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import pdb\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build lstm and Initialize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm(file_path, epochs):\n",
    "    # Get text file from path\n",
    "    print (file_path)\n",
    "    text = open(file_path).read().lower()\n",
    "    len_text = len(text)\n",
    "    print ('Text info: len {}, type {}'.format(len_text, type(text)))\n",
    "\n",
    "    # Get Unique chars from text\n",
    "    chars = sorted(list(set(text)))\n",
    "    len_chars = len(chars)\n",
    "    print ('Total Unique Chars: ', len_chars)\n",
    "\n",
    "    # Set up translation dicts\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    # fchollet = 40 => Ours = ~100\n",
    "    # rows are of size maxlen\n",
    "    maxlen = 80\n",
    "    # step through text file\n",
    "    step = 3\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        end_index = i + maxlen\n",
    "        sentences.append(text[i: end_index])\n",
    "        next_chars.append(text[end_index])\n",
    "    print ('Total number sequences: ', len(sentences))\n",
    "\n",
    "    # Start making your sparse matrices\n",
    "    print ('Vectorizing...')\n",
    "    X = np.zeros((len(sentences), maxlen, len_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len_chars), dtype=np.bool)\n",
    "\n",
    "    # Check space complexity\n",
    "    space_X = sys.getsizeof(X)\n",
    "    space_y = sys.getsizeof(y)\n",
    "    print ('Space X: {} Bytes'.format(space_X))\n",
    "    print ('Space_y: {} Bytes'.format(space_y))\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "    # Buid and run Model\n",
    "    print ('Building and Running Model ...')\n",
    "    run(X, y, len_chars, epochs, len_text, maxlen, char_indices, indices_char, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Save Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(X, y, len_chars, num_epochs, len_text, maxlen,char_indices, indices_char, text):\n",
    "    X_shape_1, X_shape_2 = X.shape[1], X.shape[2]\n",
    "    y_shape = y.shape[1]\n",
    "    full_start_time = datetime.now()\n",
    "    for epoch_counter in range(num_epochs):\n",
    "        print ('\\n Starting Epoch {} ... '.format(epoch_counter))\n",
    "        model = build_model(X_shape_1, X_shape_2, y_shape, epoch_counter - 1)\n",
    "        # Fit for for 1 epoch only\n",
    "        start_time = datetime.now()\n",
    "        # Commented out callbacks for now\n",
    "        history = model.fit(X, y, validation_split=0.20, nb_epoch=1, batch_size=512, verbose=1)\n",
    "        model_total_time = datetime.now() - start_time\n",
    "        print (\"training time: \" + str(model_total_time))\n",
    "        save_history(history)\n",
    "    \n",
    "        # Save the weights from the training\n",
    "        print ('\\n Saving weights ...')\n",
    "        model_weights = 'char_training/model_weights_' + str(epoch_counter) + '.h5'\n",
    "        model.save_weights(model_weights)\n",
    "        print ('Weights Saved')\n",
    "        gen_output(model, len_text, maxlen, len_chars, char_indices, indices_char, text, epoch_counter)\n",
    "        print ('\\n Finished output of Epoch: {}'.format(epoch_counter))\n",
    "\n",
    "    total_time = datetime.now() - full_start_time\n",
    "    print (\"Semi-total Run Time: \" + str(model_total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(X_shape_1, X_shape_2, y_shape, prev_epoch_counter):\n",
    "    # define the LSTM model via our old code - No callbacks for now\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(X_shape_1, X_shape_2), return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(y_shape, activation='softmax'))\n",
    "    \n",
    "    # Check if weights file exists, should not exist only for first run\n",
    "    file_path = 'char_training/model_weights_' + str(prev_epoch_counter) + '.h5'\n",
    "    print ('File path of model weights: ', file_path)\n",
    "    if path.isfile(file_path):\n",
    "        print ('found file, loading weights... ')\n",
    "        model.load_weights(file_path)\n",
    "    else:\n",
    "        print ('.h5 file not found')\n",
    "    # Compile and return model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_output(model, len_text, maxlen, len_chars, char_indices, indices_char, text, epoch):\n",
    "    stdout = sys.stdout\n",
    "    output_path = 'char_lstm_output_files/lstm_output_text_{:02d}.txt'.format(epoch)\n",
    "    sys.stdout = open(output_path, 'w')\n",
    "    start_index = random.randint(0, len_text - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print ('\\n Output for Epoch {:02d} with diversity {}'.format(epoch, diversity))\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print ('Seed: \"' + sentence + '\" \\n')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(1000):\n",
    "            x = np.zeros((1, maxlen, len_chars))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "    sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_history(history):\n",
    "    print (\"Saving History\")\n",
    "    with open('char_training/character-training_history.json', 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    print ('History Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Calling functions from main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/simpsons/moes_tavern_lines.txt\n",
      "Text info: len 305270, type <class 'str'>\n",
      "Total Unique Chars:  65\n",
      "Total number sequences:  101730\n",
      "Vectorizing...\n",
      "Space X: 528996128 Bytes\n",
      "Space_y: 6612562 Bytes\n",
      "Building and Running Model ...\n",
      "\n",
      " Starting Epoch 0 ... \n",
      "File path of model weights:  char_training/model_weights_-1.h5\n",
      ".h5 file not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwa\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 114s 1ms/step - loss: 3.1522 - val_loss: 2.8454\n",
      "training time: 0:01:55.455732\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 0\n",
      "\n",
      " Starting Epoch 1 ... \n",
      "File path of model weights:  char_training/model_weights_0.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 111s 1ms/step - loss: 2.5802 - val_loss: 2.2648\n",
      "training time: 0:01:53.106806\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 1\n",
      "\n",
      " Starting Epoch 2 ... \n",
      "File path of model weights:  char_training/model_weights_1.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 111s 1ms/step - loss: 2.1541 - val_loss: 2.0265\n",
      "training time: 0:01:53.090051\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 2\n",
      "\n",
      " Starting Epoch 3 ... \n",
      "File path of model weights:  char_training/model_weights_2.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 111s 1ms/step - loss: 1.9731 - val_loss: 1.8964\n",
      "training time: 0:01:53.851626\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 3\n",
      "\n",
      " Starting Epoch 4 ... \n",
      "File path of model weights:  char_training/model_weights_3.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 111s 1ms/step - loss: 1.8482 - val_loss: 1.8043\n",
      "training time: 0:01:53.416642\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 4\n",
      "\n",
      " Starting Epoch 5 ... \n",
      "File path of model weights:  char_training/model_weights_4.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 112s 1ms/step - loss: 1.7515 - val_loss: 1.7435\n",
      "training time: 0:01:53.734728\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 5\n",
      "\n",
      " Starting Epoch 6 ... \n",
      "File path of model weights:  char_training/model_weights_5.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 112s 1ms/step - loss: 1.6770 - val_loss: 1.6881\n",
      "training time: 0:01:53.749889\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 6\n",
      "\n",
      " Starting Epoch 7 ... \n",
      "File path of model weights:  char_training/model_weights_6.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 112s 1ms/step - loss: 1.6132 - val_loss: 1.6486\n",
      "training time: 0:01:53.731360\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 7\n",
      "\n",
      " Starting Epoch 8 ... \n",
      "File path of model weights:  char_training/model_weights_7.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 111s 1ms/step - loss: 1.5632 - val_loss: 1.6186\n",
      "training time: 0:01:53.370572\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 8\n",
      "\n",
      " Starting Epoch 9 ... \n",
      "File path of model weights:  char_training/model_weights_8.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 112s 1ms/step - loss: 1.5075 - val_loss: 1.6033\n",
      "training time: 0:01:53.374191\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 9\n",
      "\n",
      " Starting Epoch 10 ... \n",
      "File path of model weights:  char_training/model_weights_9.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 111s 1ms/step - loss: 1.4602 - val_loss: 1.5831\n",
      "training time: 0:01:53.117032\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 10\n",
      "\n",
      " Starting Epoch 11 ... \n",
      "File path of model weights:  char_training/model_weights_10.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 114s 1ms/step - loss: 1.4071 - val_loss: 1.5708\n",
      "training time: 0:01:56.045781\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 11\n",
      "\n",
      " Starting Epoch 12 ... \n",
      "File path of model weights:  char_training/model_weights_11.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 117s 1ms/step - loss: 1.3636 - val_loss: 1.5696\n",
      "training time: 0:01:58.637599\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 12\n",
      "\n",
      " Starting Epoch 13 ... \n",
      "File path of model weights:  char_training/model_weights_12.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 121s 1ms/step - loss: 1.3167 - val_loss: 1.5649\n",
      "training time: 0:02:02.390978\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 13\n",
      "\n",
      " Starting Epoch 14 ... \n",
      "File path of model weights:  char_training/model_weights_13.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 123s 2ms/step - loss: 1.2723 - val_loss: 1.5710\n",
      "training time: 0:02:04.699734\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 14\n",
      "\n",
      " Starting Epoch 15 ... \n",
      "File path of model weights:  char_training/model_weights_14.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 128s 2ms/step - loss: 1.2253 - val_loss: 1.5828\n",
      "training time: 0:02:09.374491\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 15\n",
      "\n",
      " Starting Epoch 16 ... \n",
      "File path of model weights:  char_training/model_weights_15.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 129s 2ms/step - loss: 1.1778 - val_loss: 1.5879\n",
      "training time: 0:02:11.005320\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 16\n",
      "\n",
      " Starting Epoch 17 ... \n",
      "File path of model weights:  char_training/model_weights_16.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 133s 2ms/step - loss: 1.1308 - val_loss: 1.5997\n",
      "training time: 0:02:14.943543\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 17\n",
      "\n",
      " Starting Epoch 18 ... \n",
      "File path of model weights:  char_training/model_weights_17.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 135s 2ms/step - loss: 1.0826 - val_loss: 1.6155\n",
      "training time: 0:02:16.319773\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 18\n",
      "\n",
      " Starting Epoch 19 ... \n",
      "File path of model weights:  char_training/model_weights_18.h5\n",
      "found file, loading weights... \n",
      "Train on 81384 samples, validate on 20346 samples\n",
      "Epoch 1/1\n",
      "81384/81384 [==============================] - 137s 2ms/step - loss: 1.0410 - val_loss: 1.6436\n",
      "training time: 0:02:18.698604\n",
      "Saving History\n",
      "History Saved\n",
      "\n",
      " Saving weights ...\n",
      "Weights Saved\n",
      "\n",
      " Finished output of Epoch: 19\n",
      "Semi-total Run Time: 0:02:18.698604\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        files = [\"data/simpsons/moes_tavern_lines.txt\"]\n",
    "        epochs = 20\n",
    "        lstm(files[0], epochs)\n",
    "    except Exception as e:\n",
    "        #raise e\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
